Test grammar a^n b^n

5-5 eventual convergence  -> "DONE"  Full dataset
5-5 eventual convergence  ->  2000 Dataset

convergence and the size of the dataset  -> "DONE"

convergence and the size of the hidden layer ->"DONE"

convergence and the size of the embeddings layer

Sample errors of a well trained network
-----------------------------------------



GRAMMAR2

convergence and the size of the dataset  - train and test

Errors on the small dataset
1000
1500 
2000
5000


Resulting preditcions for long train 5-5 eventuall start 5000
Resulting predictions for long train 5-5 eventuall start 1500

--------------------------------------------------
Eventual Convergence check

a^n b^n
5-5 eventual convergence  -> "DONE"  Full dataset 120 000 
5-5 eventual convergence  ->  2000 Dataset   120 000 (run length not known)
----------------------------------------------------
GRAMMAR
5-5 eventual convergence (5-5) 5000 ->  300000  1h  done
5-5 eventual convergence  (5-5) 2000 ->  300000 1h
5-5 eventual convergence  (5-5) 1000 ->  300000 1h
5-5 eventual convergence (5-5) 100 -> 300000 1h






Sorting length discussion (maybe) 
Learning relevant bigrams (Advesarial miniset)
